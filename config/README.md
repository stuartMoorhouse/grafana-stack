# Config Directory

This directory contains pre-built Grafana dashboard definitions for the observability stack. These files are loaded into the EKS cluster as Kubernetes ConfigMaps by Terraform, then automatically discovered and imported by the Grafana sidecar container.

## How It Works

1. Terraform reads each JSON file from `config/dashboards/` and creates a ConfigMap in the `monitoring` namespace.
2. Each ConfigMap is labeled with `grafana_dashboard: "1"` so the Grafana sidecar detects it.
3. The Grafana sidecar container (deployed as part of kube-prometheus-stack) watches for ConfigMaps with that label and provisions the dashboards automatically.
4. Dashboards appear in Grafana under the configured folder without any manual import.

## Data Source References

All dashboards reference three provisioned data sources by UID:

| Data Source | Type       | UID          |
|-------------|------------|--------------|
| Prometheus  | prometheus | `prometheus` |
| Loki        | loki       | `loki`       |
| Tempo       | tempo      | `tempo`      |

These UIDs must match the data source provisioning in the kube-prometheus-stack Helm values.

## Dashboard Files

### dashboards/kubernetes-cluster-overview.json

**UID**: `k8s-cluster-overview`

Cluster-level health overview. Shows node count, total running pods, pods not ready, CPU and memory utilization by node, pod counts by namespace (bar gauge), and cluster CPU/memory request ratios vs allocatable capacity (gauge panels). Includes a namespace dropdown variable for filtering.

### dashboards/node-exporter.json

**UID**: `node-exporter`

Per-node hardware metrics from the Prometheus node-exporter. Panels cover CPU usage %, memory usage %, root filesystem disk usage %, disk I/O read/write throughput, and network receive/transmit bandwidth. Includes an instance selector variable to filter by node.

### dashboards/pod-overview.json

**UID**: `pod-overview`

Pod-level resource monitoring. Includes CPU and memory usage time series, pod restart counts (stat panel with color thresholds at 3 and 5 restarts), a pod status table showing phase (Running/Pending/Failed with color-coded cells), and container-level CPU and memory usage plotted against resource limits (dashed limit lines). Variables: namespace, pod (multi-select).

### dashboards/loki-log-explorer.json

**UID**: `loki-log-explorer`

Searchable log viewer powered by Loki. Features a log volume histogram (stacked bar chart showing log lines per minute by pod) and a full logs panel with timestamps, log details, and line wrapping. Variables: namespace, pod, container (all from Loki label values), and a free-text search box that filters with LogQL `|= "search"`.

### dashboards/tempo-trace-explorer.json

**UID**: `tempo-trace-explorer`

Trace search and inspection powered by Tempo. Includes a trace count time series (from Tempo service graph metrics), a trace search results table with clickable trace IDs that link to the trace detail view, a traces panel for waterfall visualization, and a service latency chart showing p50/p95/p99 from Tempo service graph histogram metrics. Variable: service name (from `tempo_service_graph_request_total` labels).

### dashboards/red-golden-signals.json

**UID**: `red-golden-signals`

RED (Rate, Errors, Duration) methodology dashboard for Online Boutique services. Organized into collapsible rows:

- **Rate**: Request rate per second by service (time series).
- **Errors**: Error rate as percentage of total requests, with a 5% threshold line (time series).
- **Duration**: Separate p50, p90, p99 latency panels (time series), with the p99 panel showing a 2-second threshold.
- **Summary**: Stat panels showing current values for request rate, error rate, and p99 latency per service.

Uses `tempo_service_graph_request_total`, `tempo_service_graph_request_failed_total`, and `tempo_service_graph_request_server_seconds_bucket` metrics. Variables: namespace (default `boutique`), service (multi-select).

### dashboards/service-dependency-map.json

**UID**: `service-dependency-map`

Visualizes inter-service communication patterns. Features:

- **Node graph panel**: Shows service-to-service connections using `tempo_service_graph_request_total` and `tempo_service_graph_request_failed_total` metrics.
- **Request rate by edge**: Time series of requests per second for each client-server pair.
- **Error rate by edge**: Time series of error percentage for each client-server pair.
- **p95 latency by edge**: Server-side latency for each connection.
- **Summary table**: Tabular view of all service connections with request rate, error rate (color-coded), and p95 latency.

Uses `tempo_service_graph_*` metrics generated by Tempo's service graph processor.

## Modifying Dashboards

To update a dashboard:

1. Edit the JSON file in this directory.
2. Run `terraform apply` to update the ConfigMap.
3. The Grafana sidecar will detect the change and reload the dashboard.

Alternatively, edit dashboards in the Grafana UI and export the updated JSON back to this directory for persistence.
